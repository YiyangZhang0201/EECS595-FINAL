{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f6a85a-825f-49b5-9b4e-b8707ad0e070",
   "metadata": {},
   "source": [
    "## This is the notebook of python crawler and get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd01676-ec3c-4711-a301-cb013c804f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import lxml\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bfd85db-074f-44aa-9339-2d524933d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stock_list = [\"600000\", \"600009\", \"600016\", \"600028\", \"600030\", \"600031\",\n",
    "              \"600036\", \"600048\", \"600050\", \"600104\", \"600196\", \"600276\",\n",
    "              \"600309\", \"600438\", \"600519\", \"600547\", \"600570\", \"600585\",\n",
    "              \"600588\", \"600690\", \"600703\", \"600745\", \"600809\", \"600837\",\n",
    "              \"600887\", \"600893\", \"600918\", \"601012\", \"601066\", \"601088\",\n",
    "              \"601138\", \"601166\", \"601211\", \"601288\", \"601318\", \"601336\",\n",
    "              \"601398\", \"601601\", \"601628\", \"601668\", \"601688\", \"601818\",\n",
    "              \"601857\", \"601888\", \"601899\", \"601995\", \"603259\", \"603288\",\n",
    "              \"603501\", \"603986\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b77704-93af-4550-9fa8-f433c5bb2373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getallpage(url):\n",
    "    \"\"\"\n",
    "    Get the number of pages, which will make us possible to get all the news related to it.\n",
    "    \"\"\"\n",
    "    pagedata = requests.get(url).content.decode(\"gbk\")\n",
    "    mytree = lxml.etree.HTML(pagedata)\n",
    "    if pagedata.find(\"page_newslib\"):\n",
    "        data = mytree.xpath(\"//*[@class=\\\"page_newslib\\\"]//a[last()-1]/text()\")\n",
    "        return data\n",
    "    else:\n",
    "        return ['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e271b219-f809-4127-ba8a-49254d26ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PythonCrawler(stock_id):\n",
    "    stock_url = \"http://stock.jrj.com.cn/share,\" + stock_id + \",ggxw.shtml\"\n",
    "    # get total page number\n",
    "    total_page = int(getallpage(stock_url)[0])\n",
    "    # for first page\n",
    "    responce = urllib.request.urlopen(stock_url)\n",
    "    content = responce.read()\n",
    "    parsed = BeautifulSoup(content, \"html.parser\")\n",
    "    # get news and time\n",
    "    data = parsed.find_all('ul')[-1]\n",
    "    data1 = list(data)\n",
    "    NEWS = pd.DataFrame(columns = [\"Time\", \"Content\"])\n",
    "    for news in tqdm(data1):\n",
    "        indv_news = str(news).replace('\\n', '').split('target=\"_blank\">')[-1].replace('</i></li>', '')\n",
    "        indv_news_ct = indv_news.split('</a></span><i>')\n",
    "        if indv_news_ct != ['']:\n",
    "            indv_news_c = indv_news_ct[0]\n",
    "            indv_news_t = indv_news_ct[1].split(' ')[0]\n",
    "            NEWS = NEWS.append({\"Time\": indv_news_t, \"Content\": indv_news_c}, ignore_index=True)\n",
    "    # for other pages:\n",
    "    if total_page != 1:\n",
    "        for page in tqdm(range(2, total_page+1)):\n",
    "            stock_url = \"http://stock.jrj.com.cn/share,\" + stock_id + \",ggxw_\" + str(page) +\".shtml\"\n",
    "            responce = urllib.request.urlopen(stock_url)\n",
    "            content = responce.read()\n",
    "            parsed = BeautifulSoup(content, \"html.parser\")\n",
    "            # get news and time\n",
    "            data = parsed.find_all('ul')[-1]\n",
    "            data1 = list(data)\n",
    "            for news in tqdm(data1):\n",
    "                indv_news = str(news).replace('\\n', '').split('target=\"_blank\">')[-1].replace('</i></li>', '')\n",
    "                indv_news_ct = indv_news.split('</a></span><i>')\n",
    "                if indv_news_ct != ['']:\n",
    "                    indv_news_c = indv_news_ct[0]\n",
    "                    indv_news_t = indv_news_ct[1].split(' ')[0]\n",
    "                    NEWS = NEWS.append({\"Time\": indv_news_t, \"Content\": indv_news_c}, ignore_index=True)\n",
    "    \n",
    "    NEWS.to_csv(f\"{stock_id}NLPChinese.csv\", index=False, encoding=\"utf_8_sig\")\n",
    "    return NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830d6d7-142d-4653-b791-bdd385e902f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for stock in tqdm(Stock_list):\n",
    "    PythonCrawler(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463f8b8-25f0-49ea-91c5-e27bc4970f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
